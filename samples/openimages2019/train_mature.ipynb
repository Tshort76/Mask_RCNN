{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on OpenImages Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T18:46:45.906576Z",
     "start_time": "2019-08-20T18:46:42.648387Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "from openimages2019 import setup as st\n",
    "from openimages2019 import utils as u\n",
    "\n",
    "from skimage.draw import rectangle\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"../data\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(DATA_DIR, \"mask_rcnn_coco.h5\")\n",
    "\n",
    "#Make GPUs visible\n",
    "!export HIP_VISIBLE_DEVICES=0,1,2,3\n",
    "\n",
    "\n",
    "\n",
    "import mlflow\n",
    "#add mlflow stuff\n",
    "\n",
    "MLFLOW_SERVER = 'occ01ap200.na.simplot.com'\n",
    "\n",
    "os.environ['NO_PROXY'] = MLFLOW_SERVER\n",
    "mlflow.tracking.set_tracking_uri('http://' + MLFLOW_SERVER + ':5005')\n",
    "EXPERIMENT_NAME = 'kaggle_openimage_mask_rcnn_v1.0'\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# os.environ['AZURE_STORAGE_ACCESS_KEY'] = ''\n",
    "\n",
    "\n",
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T18:46:46.313773Z",
     "start_time": "2019-08-20T18:46:46.164872Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TrainConfig(Config):\n",
    "    \n",
    "    NAME = \"kaggle\"\n",
    "    GPU_COUNT = 2\n",
    "    IMAGES_PER_GPU = 2\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    IMAGE_MAX_DIM = 512\n",
    "    STEPS_PER_EPOCH = 500\n",
    "    \n",
    "\n",
    "def log_params(pz):\n",
    "    for k,v in pz.items():\n",
    "        mlflow.log_param(k,v)\n",
    "\n",
    "def get_infer_model(config, model_path=None):\n",
    "    inf_model = modellib.MaskRCNN(mode=\"inference\", config=config, model_dir=MODEL_DIR)\n",
    "\n",
    "    if model_path is None:\n",
    "        model_path = inf_model.find_last()\n",
    "        \n",
    "    inf_model.load_weights(model_path, by_name=True)\n",
    "    \n",
    "    return inf_model\n",
    "        \n",
    "\n",
    "def train(model, inf_config, train_data, val_data, params):\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        model.train(train_data, val_data, \n",
    "                    learning_rate=params['learning_rate'], \n",
    "                    epochs=params['epochs'], \n",
    "                    layers=params['layers'])\n",
    "\n",
    "        log_params(params)\n",
    "        \n",
    "        #Inference to get mAP\n",
    "        inf_model = modellib.MaskRCNN(mode=\"inference\", config=inf_config, model_dir=MODEL_DIR)\n",
    "        model_path = inf_model.find_last()\n",
    "        inf_model.load_weights(model_path, by_name=True)\n",
    "        mAP = u.eval_mAP(inf_model, val_data, inf_config, params['mAP_sample_size'])\n",
    "        \n",
    "        mlflow.log_metric('mAP', mAP)\n",
    "        mlflow.log_param('Model Path', model_path)\n",
    "        \n",
    "        # pickle.dump(clf,open(model_filepath,'wb'))\n",
    "        # mlflow.log_artifact(model_filepath)\n",
    "        \n",
    "        return model_path, mAP\n",
    "\n",
    "    \n",
    "    \n",
    "def train_on_class_subset(class_set, params, init_with=\"coco\"):\n",
    "    \n",
    "    class KaggleConfig(TrainConfig):\n",
    "        NUM_CLASSES = len(class_set) + 1 # + 1 for background class\n",
    "\n",
    "    class InferenceConfig(KaggleConfig):\n",
    "        GPU_COUNT = 1\n",
    "        BATCH_SIZE = 10\n",
    "\n",
    "    train_config = KaggleConfig()\n",
    "    inf_config = InferenceConfig()    \n",
    "\n",
    "    model = modellib.MaskRCNN(mode=\"training\", config=train_config, model_dir=MODEL_DIR)\n",
    "\n",
    "    if init_with == \"imagenet\":\n",
    "        model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "    elif init_with == \"coco\":\n",
    "        model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                           exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                    \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "    elif init_with == \"last\":\n",
    "        model.load_weights(model.find_last(), by_name=True)\n",
    "    \n",
    "    anns = st.load_annotations_by_image(class_set)\n",
    "    train_data = st.load_dataset(anns, DATA_DIR, class_set, is_train=True)\n",
    "    val_data = st.load_dataset(anns, DATA_DIR, class_set, is_train=False)\n",
    "\n",
    "    return train(model, inf_config, train_data, val_data, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the classes according to frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T18:47:42.129559Z",
     "start_time": "2019-08-20T18:46:49.852176Z"
    }
   },
   "outputs": [],
   "source": [
    "conn = st._get_sql_conn()\n",
    "\n",
    "all_classes = st.load_classes(conn)\n",
    "bboxes = pd.read_sql(\"SELECT ImageID, XMax, XMin, YMin, YMax, LabelName FROM [Sandbox].[kaggle].[Combined_Set_Detection_BBox]\", conn)\n",
    "\n",
    "tmp = bboxes['LabelName'].value_counts()\n",
    "tmp = tmp.apply(np.log10)\n",
    "tmp = tmp.apply(int)\n",
    "\n",
    "class_sets = []\n",
    "\n",
    "for i in range(1,7):\n",
    "    idxs = tmp[tmp == i].index.values\n",
    "    tmp_set = all_classes[all_classes['LabelName'].isin(idxs)]  #reset_index()\n",
    "    tmp_set = tmp_set.reset_index()\n",
    "    tmp_set['LabelID'] = tmp_set.index + 1\n",
    "    class_sets.append(tmp_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train a model on each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:41:17.963477Z",
     "start_time": "2019-08-19T17:41:09.169Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params = {'learning_rate' : 0.001,\n",
    "          'epochs' : 25,\n",
    "          'layers' : 'heads',\n",
    "          'mAP_sample_size' : 250\n",
    "         }\n",
    "\n",
    "#########\n",
    "#TODO log training time as well\n",
    "##########\n",
    "for i,class_set in enumerate(class_sets):\n",
    "    params['class_set_index'] = i\n",
    "    x = train_on_class_subset(class_set, params)\n",
    "    print(x)\n",
    "\n",
    "#see mlflow for results\n",
    "\n",
    "# anns = st.load_annotations_by_image(class_set)\n",
    "# train_data = st.load_dataset(anns, DATA_DIR, class_set, is_train=True)\n",
    "# val_data = st.load_dataset(anns, DATA_DIR, class_set, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models for each class set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T18:52:49.353746Z",
     "start_time": "2019-08-20T18:52:49.336908Z"
    }
   },
   "outputs": [],
   "source": [
    "background = ['/mnull','Background']\n",
    "    \n",
    "z = [np.insert(cset[['LabelName','LabelDescription']].values,0,background,axis=0) for cset in class_sets]\n",
    "\n",
    "omni_class_set = pd.DataFrame(np.concatenate(z),columns=['LabelName','LabelDescription'])\n",
    "\n",
    "id_offsets = omni_class_set[omni_class_set['LabelName'] == '/mnull'].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T18:54:15.564880Z",
     "start_time": "2019-08-20T18:53:00.845823Z"
    }
   },
   "outputs": [],
   "source": [
    "model_paths = ['kaggle20190815T2120/mask_rcnn_kaggle_0025.h5','kaggle20190815T2229/mask_rcnn_kaggle_0025.h5',\n",
    "               'kaggle20190815T2347/mask_rcnn_kaggle_0025.h5','kaggle20190816T0115/mask_rcnn_kaggle_0025.h5',\n",
    "               'kaggle20190816T0256/mask_rcnn_kaggle_0025.h5','kaggle20190816T0441/mask_rcnn_kaggle_0100.h5']\n",
    "\n",
    "models = []\n",
    "\n",
    "for i,mpath in enumerate(model_paths):\n",
    "    \n",
    "    class KaggleConfig(TrainConfig):\n",
    "        NUM_CLASSES = len(class_sets[i])+ 1 # + 1 for background class\n",
    "\n",
    "    class InferenceConfig(KaggleConfig):\n",
    "        GPU_COUNT = 1\n",
    "        IMAGES_PER_GPU = 2  #TODO mod this if needed\n",
    "\n",
    "    inf_config = InferenceConfig()\n",
    "\n",
    "    models.append(get_infer_model(inf_config,model_path=os.path.join(MODEL_DIR, mpath)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T18:55:23.921973Z",
     "start_time": "2019-08-20T18:54:34.157924Z"
    }
   },
   "outputs": [],
   "source": [
    "import skimage\n",
    "\n",
    "# test on a few random images\n",
    "# image_ids = np.random.choice(dataset_val.image_ids, inference_config.BATCH_SIZE)\n",
    "\n",
    "# images = [skimage.io.imread(os.path.join(DATA_DIR, rel_path)) for rel_path in ]\n",
    "\n",
    "#TODO should store image id in result as well ... this is just a quick hack\n",
    "\n",
    "img = skimage.io.imread(os.path.join(DATA_DIR, 'train/2fef4dd2f83feb18.jpg'))\n",
    "img2 = skimage.io.imread(os.path.join(DATA_DIR, 'train/55dee1384cd565ee.jpg'))\n",
    "\n",
    "images = [img,img2]\n",
    "\n",
    "#To store each models result for each image\n",
    "all_results = [[] for x in images]\n",
    "\n",
    "for midx,model in enumerate(models):\n",
    "\n",
    "    results = model.detect(images, verbose=0)\n",
    "    \n",
    "    for i in range(len(results)):\n",
    "        r = results[i]\n",
    "        \n",
    "        r['model_id'] = midx\n",
    "        r['class_ids'] += id_offsets[midx]\n",
    "        \n",
    "        all_results[i].append(r)\n",
    "        \n",
    "        visualize.display_instances(images[i], r['rois'], r['masks'], r['class_ids'], omni_class_set['LabelDescription'].values, r['scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non maximum suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T18:57:34.332895Z",
     "start_time": "2019-08-20T18:57:34.326295Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#assembles the results from individual models exec\n",
    "def assemble(ind_results, iou_threshold=0.3):\n",
    "    \"\"\" Combines the results from many different models across a single images.  Uses NMS to handle overlaps\"\"\"\n",
    "\n",
    "    classes = np.concatenate([x['class_ids'] for x in ind_results])\n",
    "    scores = np.concatenate([x['scores'] for x in ind_results])\n",
    "\n",
    "    rois = np.concatenate([x['rois'] for x in ind_results],axis = 0)\n",
    "\n",
    "    #TODO just to display stuff ... not needed here\n",
    "    masks = np.concatenate([x['masks'] for x in ind_results],axis = 2)\n",
    "\n",
    "    to_keep = utils.non_max_suppression(rois, scores, iou_threshold)\n",
    "\n",
    "    return {'class_ids' : classes[to_keep], 'rois' : rois[to_keep],\n",
    "          'scores' : scores[to_keep], 'masks' : masks[:,:,to_keep]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T18:57:38.837940Z",
     "start_time": "2019-08-20T18:57:37.254373Z"
    }
   },
   "outputs": [],
   "source": [
    "r = assemble(all_results[0])\n",
    "\n",
    "visualize.display_instances(img, r['rois'], r['masks'], r['class_ids'], omni_class_set['LabelDescription'].values, r['scores'])\n",
    "\n",
    "# for result in all_results:\n",
    "#     r = assemble(result)\n",
    "\n",
    "#     visualize.display_instances(img, r['rois'], r['masks'], r['class_ids'], omni_class_set['LabelDescription'].values, r['scores'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "328px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
